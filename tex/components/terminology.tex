% Version: final

\documentclass[../main.tex]{subfiles}

\begin{document}

% Definitions, these do need to be defined in the text too the first time
Throughout this study we use terminology from varying sources.
We borrow terms from cell biology and information theory.
The following is an overview of used terms and abbreviations in this paper, along with their meaning:

\begin{itemize}
\item[] (Dyadic) Between two variables.
\item[] (BRM) Biological Random Motif, a GRN motif randomly generated following rules mimicking real GRN motifs.
\item[] (Full mutual information profile) A complexity profile that shows the average mutual information at each possible emergence level of the predictor system with the predicted system.
\item[] (GRN) Gene Regulation Network, a system of genes operating at an expression level influencing each other through regulation and stimulation.
\item[] (Monadic) Within a single variable.
\item[] (PID) Partial Information Decomposition, the decomposition of information in a system in synergy, redundancy and unique information.
\item[] (Polyadic) Between multiple (more than two) variable.
\item[] (URM) Uniform Random Motif, a transition table generated using a completely random design.
\end{itemize}

% Math notation
We use the following notation throughout this paper. Let
\begin{itemize}
\item[] $n$: the number of genes in a system
\item[] $l$: the number of states a gene can be in
\item[] $\epsilon$: the magnitude of a nudge
\item[] $w$: the width of a nudge, i.e. the number of genes affected
\item[] $\mathbf{X}^{t=0}$: the set of all $n$ genes in a motif at time step $t=0$
\item[] $X_i^{t=0}$: the $i$-th gene in the motif at time step $t=0$
\item[] $\mathbf{Y}$: general notation for the set of all predicted target variables, generally $\mathbf{Y} = \mathbf{X}^{t=1}$
\end{itemize}
All random variables are discrete with $l$ states, a logarithm is base 2 unless implied otherwise, and calculations of information theory properties are in bits.
We denote the entropy of a system as $\mathrm{H}(\mathbf{X})$, the mutual information as $\mathrm{I}(\mathbf{X}:\mathbf{Y})$, the redundancy as $\mathrm{I}_\mathrm{red}(\mathbf{X}:\mathbf{Y})$ and the synergy as $\mathrm{I}_\mathrm{syn}(\mathbf{X}:\mathbf{Y})$.

\end{document}

