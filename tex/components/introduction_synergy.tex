% Version: 0.0

\documentclass[../main.tex]{subfiles}

\begin{document}

% Set the stage
% Something about complexity
In the analysis of complex system, it is helpful to have a quantification of how 'complex' a system is.
Ideally, this quantification allows for the distinction of regular systems, chaotic systems, and systems that show complex behavior.
There is no general consensus yet of how to model complexity, only that complexity should be a convex function between order and chaos \cite{bar2013computationally}.
Information theory has grown to be a staple tool in many fields that work with complex systems \cite{williams2010nonnegative}. % Referenced to later (a)
Originally, the primarily used concepts where mutual information and entropy.
These properties are not always useful in assessing system complexity; in ecology, entropy has been proposed and since gone out of favor as a quantification of system complexity.
Another concept is the Langton parameter, which can be used to find cellular automatas that exihibit complex behavior, and separate complexity from chaos and regularity \cite{langton1990computation}.
In more recent years, new quantities have been proposed in information theory, such as synergy.
Synergy was found to be a better predictor than the Langton parameter for system complexity \cite{9999QuaxChli}
Other proposals have been made to quantify complexity, such as the measure
%
\begin{equation}
C(X) = \mathrm{H}(X) - \sum_{j=1}^n \mathrm{H}(X_j^1 | X - X_j^1)
\end{equation}
%
proposed to identify the functional integration and specialization within a neural network \cite{tononi1999measures}. This measure is zero for a disconnected network, and high if much of the entropy of the system is accounted for by interactions among the system elements.
The general consensus is that complexity is strongly dependent on scale 
There is no general consensus yet of how to model complexity, only that complexity should be a convex function between order and chaos \cite{bar2013computationally}.

% Give different IT ideas
Basic principles in information theory are the entropy, mutual information and conditional entropy.
The principles are widely accepted an applied, and operate at the single- and bivariate level.
Originally, two extensions past the bivariate level were proposed.
The first was the total correlation, a single number that quantifies the total amount of redundancy between a set of random variables \cite{watanabe1960information}. 
The measure does not contain information on the structure of the system of random variables, and is related to the Kullback-Leibler divergence.
The second proposed measure was the interaction information \cite{mcgill1954multivariate}. This measure goes beyond second-order quantifications such as mutual information, and expresses the amount of synergy and mutual information in a set of variables beyond the pairwise mutual information in this system. 
Unfortunately, this measure can become negative, making it less intuitive to interpret than mutual information or entropy.
There is no general consensus on which measure is superior as of yet\cite{williams2010nonnegative}.
More recent approaches often build on on of these two ideas.
For instance, recently a system was proposed in which the reduncdancy, synergy and entropy are broken down in multivariate cases \cite{williams2010nonnegative}.
Here, a large system is essentially broken down in all possible redunancy, synergy and entropy pieces in a system (essentially all possible fields in a Venn diagram).
These are all quantified to give an overall insight in the structure of the system.
A weakness of this system is that beyond a few variables this system explodes computationally.

% Level 1: entropy
As hinted, we can look at systems of random variables at varying levels.
At the single variable level, we can examine the amount of entropy in random variable.
This is usually one through Shannon's measure for entropy entropy \cite{shannon1949mathematical}.
This is defined as 
%
\begin{equation}
\mathrm{H}(X) = -\sum^n_{i=1} P(x_i) log_b P(x_i)
\end{equation}
%
for a random variable $X$. 
This measure is maximized if the PDF is as evenly spread out as possible, in the case of a discrete PDF when all probabilities are uniform.

% Level 2: mutual information
At the bivariate level, we can examine the overlap in information between the two random variables.
This is quantified using the mutual information, which is defined as 
%
\begin{equation}
\mathrm{I}(X;Y) = \sum_{y \subset Y} \sum_{x \subset X} \log_b (\frac{p(x,y)}{p(x) p(y)})
\end{equation}
%
for random variables $X$ and $Y$ \cite{cover2012elements}.
When dealing with continuous probability distributions, an integral is used instead of a summation.

% Level 3: synergy/redundancy
% Give meaning to what synergy is
We can also consider a third variable in our system.
In this case, usually the third variable is seen as an 'output' variable, and it is observed how much information the former two contain about the third.
In this decomposition, we attempt to split the mutual information $I(Z;X,Y)$ into four categories \cite{williams2010nonnegative}:
%
\begin{enumerate}
\item Redundant information contained in both $X$ and $Y$, denoted $I_\mathrm{red}(Z;X,Y)$
\item Information solely contained in $X$, denoted $I(Z; X) - I_\mathrm{red}(Z;X,Y)$
\item Information solely contained in $X$, denoted $I(Z; Y) - I_\mathrm{red}(Z;X,Y)$
\item Synergetic information contained in neither $X$ and $Y$, denoted $I_\mathrm{syn}(Z;X,Y)$
\end{enumerate}

Following this split, the synergy is defined as
%
\begin{equation}
\mathrm{I}_\mathrm{syn}(Z;X,Y) = \mathrm{I}(Z;X, Y) - \mathrm{I}(Z; X) - \mathrm{I}(Z; Y) + \mathrm{I}_\mathrm{red}(Z;X,Y)
\end{equation}

The question is how to quantify either the synergetic information, or the redundant information; once one is found, the other follows.
Some quantifications have been proposed, such as the relatively simple redundancy measure
%
\begin{equation}
\mathrm{I}_\mathrm{red} = \sum_{j=1}^n [\mathrm{I}^\mathrm{P}(X_j;Y)] - \mathrm{I}^\mathrm{P}(X;Y)
\end{equation}
%
where $\mathrm{I}^\mathrm{P}(X;Y)$ represents the mutual information between $X$ and $Y$ after $X$ has been injected with a fixed amount of random noise, to turn the static system into a system of dependent PDFs \cite{tononi1999measures}.
This quantifications measures whether the sum of the mutual information measures between the elements of $X$ with $Y$ is higher than the total mutual information between $X$ and $Y$, annd is paired with a degeneracy quantification by the author, designed to measure if the mutual information between increasingly large subsets of $X$ and $Y$ scales linearly with the subset size.
In more recent years, redundancy measures have become more advanced, such as the minimal information, based on projections on probability distributions \cite{harder2013bivariate}.
The quantification of one of the two is critical, as without this it is not possible to discern between synergy and redundancy when looking at the mutual information in three variable system.

% Piece about synergistic function, intuition from discrete to continuous
Synergy can be shown intuitively in discrete cases through an X-OR gate, which is fully synergistic \cite{quax2017quantifying}.
We can demonstrate this by examining the truth table, as shown in Table \ref{XOR}.
The mutual information $\mathrm{I}(X;Z)$ and $\mathrm{I}(Y;Z)$ are both zero, but together $X$ and $Y$ provide information about $Z$.

\begin{table}
\begin{center}
\begin{tabular}{|c|c||c|}
\hline
$X$ & $Y$ & $Z$ \\
\hline
\hline
1 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 1 \\
0 & 0 & 0 \\
\hline
\end{tabular}
\end{center}
\caption{Truth table of an X-OR gate}
\label{XOR}
\end{table}

And attractive example in the continuous realm is that of bi-fan motif, where input variables $X$ and $Y$ are both promotors of variables $A$ and $B$, but where $B$ is a strong inhibitor of $A$ when its production is promoted by both $A$ and $B$ (Figure \ref{bifan_syn}).
If we know whether the $X$ is of a high concentration, we do not know if $A$ will be too, as we do not know if $Y$ is present in high enough concentrations to cause inhibition of the production of $A$.
The same is true of the concentration of $Y$; only when we know both, we obtain information of $A$.
This creates a similar situation as an X-OR gate in a continuous setting, as the network motifs can be modelled in the form of an ODE system.

%% PICTURE %%
\begin{figure}
\begin{center}
\begin{tikzpicture}[node distance=2cm,>=stealth',bend angle=45,auto]

  \tikzstyle{place}=[circle,thick,draw=blue!75,fill=blue!20,minimum size=6mm]
  \tikzstyle{transition}=[rectangle,thick,draw=black!75,
  			  fill=black!20,minimum size=4mm]

  \tikzstyle{every label}=[black]

  \begin{scope}
    % First net
    \node [place,tokens=1]                  (r1) [label=above:$X$]             {};
    \node [place,tokens=1]                  (r2) [right of=r1,label=above:$Y$] {};
    \node [transition,tokens=1]             (p1) [below of=r1,label=below:$A$] {}
      edge [pre, line width=0.5mm]          (r1)
      edge [pre, line width=0.5mm]          (r2);
    \node [transition]                      (p2) [below of=r2,label=below:$B$] {}
      edge [pre]                            (r1)
      edge [pre]                            (r2)
      edge [post, dotted, line width=0.5mm] (p1);
   \end{scope}
\end{tikzpicture}
\end{center}
\caption{Bi-fan network with additional inhibition element, dots indicate variables captured in model}
\label{bifan_syn}
\end{figure}

\end{document}