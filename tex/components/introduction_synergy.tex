% Version: 0.0

\documentclass[../main.tex]{subfiles}

\begin{document}

\subsubsection{Quantifying complexity}
% Set the stage
% Something about complexity

In the analysis of complex system, it is helpful to have a quantification of how 'complex' a system is.
Ideally, this quantification allows for the distinction of regular systems, chaotic systems, and systems that show complex behavior.
There is no general consensus yet of how to model complexity, only that complexity should be a convex function between order and chaos \cite{bar2013computationally}.
Information theory (IT) tackles this problem, and has grown to be a staple tool in many fields that work with complex systems \cite{williams2010nonnegative}. % Referenced to later (a)
Originally, the primarily used concepts where mutual information and entropy.
These properties are not always useful in assessing system complexity; in ecology, entropy has been proposed and since gone out of favor as a quantification of system complexity.
Another concept is the Langton parameter, which can be used to find cellular automatas that exihibit complex behavior, and separate complexity from chaos and regularity \cite{langton1990computation}.
In more recent years, new concepts from IT have been proposed in information theory, such as synergy.
Synergy was found to be a better predictor than the Langton parameter for system complexity \cite{9999QuaxChli}.
Other proposals have been made to quantify complexity, such as the measure
%
\begin{equation}
C(X) = \mathrm{H}(X) - \sum_{j=1}^n \mathrm{H}(X_j^1 | X - X_j^1)
\end{equation}
%
proposed to identify the functional integration and specialization within a neural network \cite{tononi1999measures}.
These quantifications are typically built on IT concepts such as entropy and mutual information.
This particular measure is zero for a disconnected network, and high if much of the entropy of the system is accounted for by interactions among the system elements.

\subsubsection{Partial information decomposition}
% Discuss the current dominant paradigm: PID

% First the far history, with unsuccesful attempts
Basic principles in IT are the (conditional) entropy and mutual information, measured in bits.
The principles are widely accepted an applied, and operate at the monadic and dyadic level.
Originally, two extensions that support polyadic interactions were proposed.
The first was the total correlation, a single number that quantifies the total amount of redundancy between a set of random variables \cite{watanabe1960information}. 
The measure does not contain information on the structure of the system of random variables, and is related to the Kullback-Leibler divergence.
The second proposed measure was the interaction information \cite{mcgill1954multivariate}. 
This measure goes beyond second-order quantifications such as mutual information, and expresses the amount of synergy and mutual information in a set of variables beyond the pairwise mutual information in this system. 
Unfortunately, this measure can become negative, making it less intuitive to interpret than mutual information or entropy.

% Current paradigm, not final
There is no general consensus on which measure is superior as of yet, or how to do a partial information decomposition (PID) \cite{griffith2011quantifying, williams2010nonnegative}.
The current dominant paradigm in IT splits the information of a system into three basic principles.
In this approach, the information in a system is broken down into redundancy, synergy and unique information \cite{williams2010nonnegative}.
All information in the system can be classified and quantified in these categories, allowing for a full decomposition of the system that improves our understanding of this system.
The unique information and redundancy are easily quantified, as they are intuitively linked to entropy and mutual information.
However, a weakness of this system is that beyond a few variables this system explodes computationally.
In addition, no measure capable of measuring synergy that satisfies all axioms has been proposed yet \cite{griffith2011quantifying}.

% Level 1: entropy
We can look at systems of random variables at varying levels.
At the single variable level, we can examine the amount of entropy in a random variable.
This is usually one through Shannon's measure for entropy entropy \cite{shannon1949mathematical}.
This is defined as 
%
\begin{equation}
\mathrm{H}(X) = -\sum^n_{i=1} P(x_i) log_b P(x_i)
\end{equation}
%
for a random variable $X$.
For a continuous distribution this definition of the entropy is replaced by differential entropy, which integrates instead of using a summation.
This measure is maximized if the probability distribution is as evenly spread out as possible, in the case of a probability mass function (PMF) when all probabilities are uniform.

% Level 2: mutual information
At the bivariate level, we can examine the overlap in information between the two random variables.
This is quantified using the mutual information, which is defined as 
%
\begin{equation}
\label{MI}
\mathrm{I}(X;Y) = \sum_{y \subset Y} \sum_{x \subset X} \log_b (\frac{p(x,y)}{p(x) p(y)})
\end{equation}
%
for random variables $X$ and $Y$ \cite{cover2012elements}.
When dealing with continuous probability distributions, an integral is used instead of a summation.
For dependent PMFs the conditional entropy can also be determined, which is expressed as 
%
\begin{equation}
\mathrm{H}(X | Y = y) = -\sum^n_{i=1} P(x_i | Y = y) log_b P(x_i | Y = y)
\end{equation}
%

% Level 3: synergy/redundancy
% Give meaning to what synergy is
We can also consider a third variable in our system.
In this case we do not only look at dyadic interactions, but also at polyadic interaction.
For instance, we might consider our first two variables as predictor variables, and our third variable as the predicted variable.
We can measure how much information the former two contain about the third with the mutual information $I(Z;X,Y)$.
To analyze this information further, we must split this information into four categories \cite{williams2010nonnegative}:
%
\begin{enumerate}
\item Redundant information contained in both $X$ and $Y$, denoted $\mathrm{I}_\mathrm{red}(Z;X,Y)$
\item Information solely contained in $X$, denoted $\mathrm{I}(Z; X) - \mathrm{I}_\mathrm{red}(Z;X,Y)$
\item Information solely contained in $X$, denoted $\mathrm{I}(Z; Y) - \mathrm{I}_\mathrm{red}(Z;X,Y)$
\item Synergetic information contained in neither $X$ and $Y$, denoted $\mathrm{I}_\mathrm{syn}(Z;X,Y)$
\end{enumerate}
This PID is also shown in Figure~\ref{venn}.
The quantification of either synergy or redundancy is critical, as without this it is not possible to discern between synergy and redundancy when looking at the mutual information in a system with $n \ge 3$.

%% PICTURE %%
\def\firstcircle{(0:-0.9cm) circle (2cm)}
\def\secondcircle{(0:0cm) circle (3cm)}
\def\thirdcircle{(0:0.9cm) circle (2cm)}

% Now we can draw the sets:
\begin{figure}[ht]
\begin{center}
\begin{tikzpicture}
    \draw \firstcircle;
    \draw \secondcircle;
    \draw \thirdcircle;
    
    \begin{scope}[fill opacity=0.5]
        \clip \firstcircle;
        \fill[orange] \thirdcircle;
    \end{scope}
    
    \begin{scope}[even odd rule, fill opacity=0.5]
        \clip \thirdcircle (-3,-3) rectangle (3,3);
        \fill[yellow] \firstcircle;
    \end{scope}
    
    \begin{scope}[even odd rule, fill opacity=0.5]
        \clip \firstcircle (-3,-3) rectangle (3,3);
        \fill[red] \thirdcircle;
    \end{scope}
    
    \begin{scope}[even odd rule, fill opacity=0.3]
        \clip \firstcircle (-4,-4) rectangle (4,4);
        \clip \thirdcircle (-4,-4) rectangle (4,4);
        \fill[blue] \secondcircle;
    \end{scope}
    
    \node (x) at (-2,0)  {$\mathrm{I}(Z;X)$};
    \node (y) at (2,0)   {$\mathrm{I}(Z;Y)$};
    \node (r) at (0,0)   {$\mathrm{I}_\mathrm{red}(Z;X,Y)$};
    \node (s) at (0,2.3) {$\mathrm{I}_\mathrm{syn}(Z;X,Y)$};
    \node (w) at (0,3.2) {$\mathrm{I}(Z;X,Y)$};
    
\end{tikzpicture}
\end{center}
\caption{Partial information-diagram showing a PID of a 3-variable system}
\label{venn}
\end{figure}
Following this split, the synergy is defined as
%
\begin{equation}
\label{red_plus_syn_is_mi}
\mathrm{I}_\mathrm{syn}(Z;X,Y) = \mathrm{I}(Z;X, Y) - \mathrm{I}(Z; X) - \mathrm{I}(Z; Y) + \mathrm{I}_\mathrm{red}(Z;X,Y)
\end{equation}

\subsubsection{Practical example of synergy}
% EXAMPLE: Piece about synergistic function, intuition from discrete to continuous

Synergy can be shown intuitively in discrete cases through an X-OR gate, which is fully synergistic \cite{quax2017quantifying}.
We can demonstrate this by examining the truth table, as shown in Table \ref{XOR}.
The mutual information $\mathrm{I}(X;Z)$ and $\mathrm{I}(Y;Z)$ are both zero, but together $X$ and $Y$ provide information about $Z$.

\begin{table}[ht]
\begin{center}
\begin{tabular}{|c|c||c|}
\hline
$X$ & $Y$ & $Z$ \\
\hline
\hline
1 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 1 \\
0 & 0 & 0 \\
\hline
\end{tabular}
\end{center}
\caption{Truth table of an X-OR gate}
\label{XOR}
\end{table}

An attractive example in the continuous realm is that of bi-fan motif, where input variables $X$ and $Y$ are both promotors of variables $A$ and $B$, but where $B$ is a strong inhibitor of $A$ when its production is promoted by both $A$ and $B$ (Figure~\ref{bifan_syn}).
If we know whether the $X$ is of a high concentration, we do not know if $A$ will be too, as we do not know if $Y$ is present in high enough concentrations to cause inhibition of the production of $A$.
The same is true of the concentration of $Y$; only when we know both, we obtain information of $A$.
This creates a similar situation as an X-OR gate in a continuous setting, as the network motifs can be modelled in the form of an ODE system.

%% PICTURE %%
\begin{figure}[ht]
\begin{center}
\begin{tikzpicture}[node distance=2cm,>=stealth',bend angle=45,auto]

  \tikzstyle{place}=[circle,thick,draw=blue!75,fill=blue!20,minimum size=6mm]
  \tikzstyle{transition}=[rectangle,thick,draw=black!75,
  			  fill=black!20,minimum size=4mm]

  \tikzstyle{every label}=[black]

  \begin{scope}
    % First net
    \node [place,tokens=1]                  (r1) [label=above:$X$]             {};
    \node [place,tokens=1]                  (r2) [right of=r1,label=above:$Y$] {};
    \node [transition,tokens=1]             (p1) [below of=r1,label=below:$A$] {}
      edge [pre, line width=0.5mm]          (r1)
      edge [pre, line width=0.5mm]          (r2);
    \node [transition]                      (p2) [below of=r2,label=below:$B$] {}
      edge [pre]                            (r1)
      edge [pre]                            (r2)
      edge [post, dotted, line width=0.5mm] (p1);
   \end{scope}
\end{tikzpicture}
\end{center}
\caption{Bi-fan network with additional inhibition element, dots indicate variables captured in model}
\label{bifan_syn}
\end{figure}

\subsubsection{Quantifying redundancy}
% Conclusion ought to be "not good enough" because they cannot be solved analytically

The problem of creating a PID can be solved by quantifying either the synergetic information or the redundant information; once one is found, the other follows.
Several quantifications have been proposed over the years.
An early attempt at a redundancy quantification was the measure
%
\begin{equation}
\mathrm{I}_\mathrm{red} = \sum_{j=1}^n [\mathrm{I}^\mathrm{P}(X_j;Y)] - \mathrm{I}^\mathrm{P}(X;Y)
\end{equation}
%
where $\mathrm{I}^\mathrm{P}(X;Y)$ represents the mutual information between $X$ and $Y$ after $X$ has been injected with a fixed amount of random noise, to turn the static system into a system of dependent PDFs \cite{tononi1999measures}.
This quantifications measures whether the sum of the mutual information measures between the elements of $X$ with $Y$ is higher than the total mutual information between $X$ and $Y$, and is paired with a degeneracy quantification by the author.
This measure has been designed to measure if the mutual information between increasingly large subsets of $X$ and $Y$ scales linearly with the subset size, and as a result will count synergy as 'negative redundancy', resulting in an underestimation of redundancy.

% Then minimal information
A more recent proposal is the minimal information $\mathrm{I}_\mathrm{min}$ \cite{williams2010nonnegative}.
This is defined as
%
\begin{equation}
\mathrm{I}_\mathrm{min}(Y;{X_1, X_2,...,X_k}) = \sum_s [p(s) \min_{X_i} [\mathrm{I}(Y=y;X_i)]]
\end{equation}
%
where $\mathrm{I}(Y = y;X)$ is the specific information.
This quantifies information related to a specific outcome, and can be reduced through summation (or integration in the continuous case) over $y$ to the mutual information.
In more recent years, this was still cited as the best redundancy measure, although it has its flaws and should be used with discretion \cite{lizier2013towards, olbrich2015information}.

% Finally bivariate redundancy
Critics of the minimal informtion have proposed an alternative based on PDF projections \cite{harder2013bivariate}.
This quantification meets all requirements posed by Williams and Beer, and meets an additional criterium that Harder et al. proposed.
The bivariate redundancy is expressed as
%
\begin{equation}
\mathrm{I}_\mathrm{biv}(Y;{X_1, X_2}) = \min [\mathrm{I}_Z^\pi (X \searrow Y), \mathrm{I}_Z^\pi (Y \searrow X)] 
\end{equation}
%
where $\mathrm{I}_Z^\pi (Y \searrow X)$ is the projected information of $Y$ on $X$, defined through the Kullback-Leibler divergence as
%
\begin{equation}
\mathrm{I}_Z^\pi (X \searrow Y) = \sum_x p(x) [D_\mathrm{KL} (p(z|x) \| p(z)) - D_\mathrm{KL} (p_{(x \searrow Y)}(z|x) \| p(z))]
\end{equation}
A key problem with these recent redundancy measures is that they cannot be solved analytically, but require numerical optimization.
This attaches a significant computational cost to any redundancy estimation.

\subsubsection{Quantifying synergy}
% SYNERGY: from old to new, see OLBRICH 2015 and GRIFFITH 2014

% Which axioms should be obeyed
For synergy, a number of measures have been proposed as well \cite{griffith2014quantifying, olbrich2015information}.

% Table

An early synergy measure is the $\mathrm{I}_\mathrm{max}$-synergy, denoted $\mathcal{S}_\mathrm{max}$ \cite{williams2010nonnegative}.
This quantity is closely related to the redundancy measure
%
\begin{equation}
\mathrm{I}_\mathrm{max} (X; Y) = \sum_{y \in Y} [ p(Y = y) \max_i \mathrm{I} (X_i ; Y = y) ]
\end{equation}
%
and is defined as 
%
\begin{equation}
\mathcal{S}_\mathrm{max} (X;Y) = \mathrm{I}(X;Y) - \mathrm{I}_\mathrm{max} (X;Y)
\end{equation}
%
This measure per definition obeys the axiom in equation~\ref{red_plus_syn_is_mi} \cite{griffith2014quantifying}.
It is said to typically overestimate synergy, and can thus function as an upper bound of sorts.

% LOWER BOUND IS NICE TO IMPLEMENT REGARDLESS, EASY TO DO
A second measure is the whole-minus-sum (WMS) synergy, a signed measure where a positive value
signifies synergy.
This can be expressed as
%
\begin{equation}
\mathrm{WMS}(X;Y) = \mathrm{I}(X;Y) - \sum_i [\mathrm{I}(X_i;Y)]
\label{WMS}
\end{equation}
%
As this is essentially the synergy minus the redundancy, the WMS-synergy can be used a lower bound for the synergy in a system \cite{griffith2014quantifying, olbrich2015information}.

For the bivariate case, the synergy from unique information can be used, $\mathcal{S}_\mathrm{vk}$ \cite{bertschinger2014quantifying, griffith2014quantifying, olbrich2015information}.
This is expressed as
\begin{equation}
\mathcal{S}_\mathrm{vk} (X;Y) = \mathrm{I}(X;Y) - \mathrm{I}_\mathrm{VK} (X;Y)
\end{equation}
%
where $\mathrm{I}_\mathrm{VK}$ is the unique information, a measure based on the Kullback-Leibler divergence.
This method is cited to be good in the bivariate case, where synergy is measured between $X$ and $Y$ when explaining $Z$ \cite{olbrich2015information}.

% Quax also has his measure
A measure of synergy based on synergistic random variables (SRVs) has also been proposed \cite{quax2017quantifying}.
This method is centered around the determination of a set of SRVs, which have zero mutual information with the individual variables in the inspected system, but a non-zero mutual information with the system as a whole.
The total synergystic information is defined as
\begin{equation}
\label{SRV}
I_\mathrm{syn}(X \rightarrow Y) \equiv \max_k \sum_i I(Y : S_{i,k}^\perp)
\end{equation}
where $S_{i,k}$ represents the $i$th SRV in the $k$th set of possible sets of SRVs.

% IGNORE CORRELATIONAL IMPORTANT, THIS IS NOT A GOOD UPPER BOUND FOR SYNERGY
% SYNERGY FROM MAX ENTROPY ARGUMENTS BECOMES NEGATIVE
Alltogether, this gives us the following summary of the available synergy measures \cite{griffith2014quantifying}:
%
\begin{equation}
\max [0,\mathrm{WMS}(X;Y)] \le \mathcal{S}_\mathrm{VK} (X;Y) \le \mathcal{S})\mathrm{max} (X;Y) \le \mathrm{I}(X;Y)
\end{equation}
%
Many of these measures rely on numerical optimization over a free parameter, like the current redundancy measures.
Notable exceptions to this rule are the WMS-synergy
Other notable synergy measures are the correlational importance, which was found to not be a good upper bound for synergy, and the synergy from maximum entropy arguments, which can take negative values \cite{griffith2014quantifying, olbrich2015information}.
\end{document}