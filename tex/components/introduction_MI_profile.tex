% Version: final

\documentclass[../main.tex]{subfiles}

\begin{document}

% Why make a profile
As discussed in the previous section, we can decompose the information of a system into three categories: synergy, redundancy and unique information.
However, synergy and redundancy can occur at many different levels.
In a 5-predictor system, information could be shared by two variables, but also by five.
Similarly, synergy could emerge from the combination of two variables, or only if all predictors are considered together.
As a result, the number of quantifiable relations explodes with a larger system size.
Even when we consider just the pairwise mutual information between all possible subsets of a network, and the output variable, we obtain $2^n$ measurements (including the empty set), which is an exponential increase.

Due to this rapid increase of the number of quantifiable relations, we would like to extract key characteristics of the system.
For instance, a system might only start showing emergent synergy when considering all predictors together, yet nothing when looking at smaller subsets.
This could be achieved by compressing this information in a visual format, that highlights interesting properties of the system as a whole, and disregards unimportant details.
This visualization can take the format of a 'profile' that is characteristic for the type of system.
When looking at complex systems, the scale is often an important factor regarding complexity, and as such it is often picked as the independent variable in profiles \cite{bar2013computationally, quax2017quantifying, tononi1999measures}.
The variable on the y-axis in the profile shows more variation, and is usually an information theory quantification that is averaged over all subsets of a specified size.

% Talk about early complexity profile
We see the first appearances of complexity profiles regarding biological system in the neurosciences.
It was investigated how redundancy is distributed over the different scales (from subset size 1 to $n$) in simple neural networks \cite{tononi1999measures}.
Several information theory quantifications were proposed as the dependent variable in these profiles, such as
%
\begin{itemize}
\item The average MI with the output $<\mathrm{I}\left( \mathbf{X}^k;Y \right) >$, with $\mathbf{X}^k$ being a subset of size $k$
\item The average shared information between the subset, the inverse set, and the output  $<\mathrm{I}\left( \mathbf{X}^k;\mathbf{X} - \mathbf{X}^k;Y\right) >$
\item The average redundancy $<\mathrm{I}_\mathrm{red}\left( \mathbf{X};Y\right) >$
\item The average system integration $<\sum_{i = 1}^{\binom{n}{k}}[\mathrm{H}(\mathbf{X}^k)] - \mathrm{H}\left( X\right) >$
\end{itemize}
%
The focus lies strongly on identifying degeneracy or complexity, as these quantities can be derived by design through integration of the complexity profile.

% Talk about pairwise complexity profile
An approach to a complexity profile is given by Bar-Yam \cite{bar2013computationally}.
He constructs a pairwise, non-negative complexity profile based on the mutual information of all variable pairs in the system.
He constructs a function $k_i (p)$ as the number of variables from $x_j \in X$ where $i \ne j$ that have a coupling higher than $p$ with variable $x_i$.
The coupling is defined as a normalized version of the mutual information $\mathrm{I}\left( x_i;x_j \right)$.
This function is inverted to obtain a variable-specific complexity
% NOTE: p is not a probability here!
\begin{equation}
\overset{~}{C}_i\left( k \right)  = p \left( k_i\right)
\end{equation}
%
which is summed and normalized to
%
\begin{equation}
C\left( k \right)  = \sum_{k^\prime= k}^m \frac{1}{k^\prime} [\overset{~}{C} \left( k^\prime \right) - \overset{~}{C} \left( k^\prime + 1 \right) ]
\end{equation}
%
with
%
\begin{equation}
\overset{~}{C} \left( k \right)  = \sum_{i=1}^n \overset{~}{C}_i \left( k \right) 
\end{equation}

As only pairs are examined, this method is computationally a lot cheaper than models that examine all possible subset sizes, costing $\binom{n}{2}$ mutual information computations.
This system is not without downsides: due to the pairwise nature higher-order relations, such as synergy, are not captured.

% Another option, that is not non-decreasing/increasing
As a response to Bar-Yam's proposal for a complexity function, an alternative that separates structure from entropy has been suggested \cite{arbona2014statistical}.
They provide a complexity function of the form
%
\begin{equation}
C\left( s\left( \hat{x},t \right) \right) = \mathrm{H}\left( s\left( \hat{x},t \right) \right) D\left( s\left( \hat{x},t \right) \right)
\end{equation}
%
where $D\left( s\left( \hat{x},t \right) \right)$ is a correlation measure, and $s\left( \hat{x},t \right) $ is the state of the system at location $\hat{x}$ and time $t$.
A complexity at time $t$ at a chosen scale level can be derived by integrating this complexity in space.
This complexity measure is especially suitable for spatial information, such as the vector fields relevant in bird flocking behavior.

% Talk about MI profile
% Details go in the methods section, here goes only what Rick said to me
A full mutual information profile, such as the one proposed by Tononi, has been proposed by Quax, where $< \mathrm{I}\left( \mathbf{X}^k;Y \right) >$ is plotted against the subset size $k$ \cite{QuaxPersonal, tononi1999measures}. 
When normalized, this would provide a non-decreasing, non-negative profile with a range from 0 to 1 that allows us to detect extreme cases of synergy and redundancy.
If there is no redundancy or synergy, a straight line would form.
If the profile is above the straight line, there is more redundancy than synergy in the system, and vice versa if the profile falls below this line, as synergy expresses itself as negative mutual information.
With this information, Quax argues that it is possible to maximum bounds to synergy and redundancy.
For instance, if the profile reaches the maximum possible value for a small subset size there cannot be synergy in the system.
In addition, this can enable one to not only attach bounds to synergy and redundancy, but also observe at what subset size-level it occurs.
This can provide valuable information about the way synergy and redundancy are incorporated in the structure of the system.

% Theoretische simpele cases uitwerken
% Belangrijk om een paar voorbeelden uit te werken met MI profielen, laat zien dat een compleet redundant variable een rechte lijn is, maar overlap erboven zit, reken in bits

\end{document}