% Version: 1.1

\documentclass[../main.tex]{subfiles}
\usepackage[]{algorithm2e}

\begin{document}

\subsection{Dataset}

We use the binary-valued Boolean model...
%TODO find dataset

We also have correlational data between gene activities, measured in their mRNA expression level.
%TODO find dataset

\subsection{Complexity Profile}

% Introduction to our synergy profile
% Link to introduction
In previous research, the mutual information within a system has been used to investigate complexity in systems of independent and dependent random variables.
A multiple mutual information-based profile has been proposed in the literature that is able to give insights beyond pairwise relations.
A synergy profile can be considered as a plot of the fraction captured of the total mutual information between all input variables and all output variables versus the number of variables taken into consideration, or
%
\begin{equation}
C_\mathrm{mult}(k) = \frac{1}{\binom{n}{k}}\frac{\sum_{X_i \in [X]^k} [\mathrm{I}(X_i;Y)]}{\mathrm{I}(X;Y)}
\end{equation}

This profile has the property $C_\mathrm{mult}(0) = 0$, as for $k = 0$ there is only the empty set, which has zero mutual information with $Y$.
In addition, we know that $C_\mathrm{mult}(n) = 1$, as this simply results in
%
\begin{align}
C_\mathrm{mult}(k) 
&= \frac{1}{\binom{n}{k}}\frac{\sum_{X_i \in [X]^k} [\mathrm{I}(X_i;Y)]}{\mathrm{I}(X;Y)} \\
&= \frac{\mathrm{I}(X;Y)}{\mathrm{I}(X;Y)} \\
&= 1
\end{align}

Finally, we can show that this profile is non-decreasing.
We can prove this by imagining an extreme case, where out of variable set $Z$ only $z_1$ provides direct information about the output variable, whereas the rest only provide information when all considered together.
When considering a subset size $1 \le k < n_z $, the complexity will be
%
\begin{equation}
C_\mathrm{mult}(k) = \frac{1}{\binom{k}{n_z}} \mathrm{I}(z_1;Y)
\end{equation}
%
where $n_z$ is the size of set $Z$.
As the mutual information term is constant for $k$, only the fraction determines the complexity value.
As with increasing $k$ fewer and fewer subsets can be made, the function will always be increasing or stagnant, the latter of which is possible only if $C_\mathrm{mult}(0) = 0$.

As there are many ways to take subsets when $1 < k < n$, we average the sum over all subsets.
However, it is also possible to rewrite this to
%
\begin{equation}
C_\mathrm{mult}(k) = \frac{\max_{X_i \in [X]^k} [\mathrm{I}(X_i;Y)]}{\mathrm{I}(X;Y)}
\end{equation}
%
to focus on extreme values within the set of subsets of size $k$.

% We mainly do the discrete case
% Go into the math behind it, using the NPEET library documentation
In the discrete case, we can simply calculate the mutual information as
%
\begin{equation}
	I(X;Y) = \sum_{y \in Y} \sum_{x \in X} p(x,y) \log (\frac{p(x,y)}{p(x)p(y)})
\end{equation}
%
We can apply this complexity measure on continuous distributions, forcing us to use approximation methods for the mutual information term.
In this scenario, we use the k-Nearest Neighbor method described by Kraskov, and implemented in Python by Ver Steeg et al. \cite{kraskov2004estimating, versteeg2013NPEET}.
For this method, a set of $N$ of samples is drawn from the underlying distributions of both the input and the output system.
This results in two matrices: an input sample matrix of size $N \times X$, and an output sample matrix of size $N \times Y$, filled with continuous values.
In both matrices, the $N$-size dimension represents samples, and the $X$- or $Y$-sized dimension represents the variables of the respective system.
These samples are used in the NPEET library for a multiple mutual information approximation.
Specifically, Kraskov's first algorithm is used, with the addition of the division by a logarithm to report answers in bits.
This is
%
\begin{equation}
\mathrm{I}_\mathrm{kNN}(X;Y) = \frac{\psi(k) - < \psi(n_x + 1) + \psi(n_y + 1) > + \psi(N)}{\log(2)}
\end{equation}
%
where $\psi(x)$ is the digamma function, and $n_x(i)$ and $n_y(i)$ respectively are the number of points within $\epsilon(i)/2$ of $x_i$ an $y_i$. Here, $\epsilon(i)/2$ is the distance to the $k$-th nearest neighbor of the point in $(X,Y)$-space.

% Subsets
We now can approximate the total mutual information between the input- and the output system.
To produce a plot, we must obtain a mutual information estimate for subsets of each size $1 \le k \le X$, versus the entire output system.
Too obtain this overall estimate for each subset size, we first find the set of mutual informations between every possible subset of $X$ of size $k$, versus the entire system $Y$.
Then, we take the average of the set and divide by the total mutual information between the input- and output system to arrive at the value that corresponds to subset size $k$ in our synergy profile.
We repeat this system for each possible subset size.
Finally, the plot is produced by plotting the $k$ against the corresponding measure.

\subsection{Simulation Methodology}
% I write this while doing my experiments, as it helps to find direction in the chaos that is scientific research
% In the end I decided to do discrete research, if I need the continuous stuff again go back to commits before Aug 2017

% General idea
In our simulation, we consider the synergetic properties, memory, and resilience of gene regulatory networks.
In a biological system, there is no clear set of input- and output variables.
However, we do have a development of the system through time.
As such, we have chosen to consider the system $A_{t=0}$ as the input system, and system $A_{t=\delta t}$ as the output system.
Here, $\delta t$ is an step in time. % in arbitrary units or in seconds, once we start using rates?
The size of the set $A$ is in theory arbitrarily big, but in reality limited by computational complexity of the evaluation of the model, which is in sections of the implementation $O(2^{c \dot n})$.

% System definition
We use a discrete model that describes a gene regulatory system.
In our experiments we use a binary system, where genes can either be turned on or off.
However, this methodology supports a larger number of expression levels as well.
The system is defined through a joint probability mass function, which represents the probability that a given state of the system occurs.
As the basis, the joint PDF framework is used \cite{jointpdf}.
In this Python framework, a joint distribution is stored as a $n$-tree, where $n$ is the number of states a variable can be found in.

% Explain how to go from correlations to PMF
The joint distribution in our case can be derived from a correlation matrix, describing the correlations between each possible pair of the variables.
This is allows for the usage of empirical data, as the correlation data is in many cases available \cite{}.
The matrix is converted to a joint PMF by assuming that the first gene has an equal chance of being in each state.
The result is a tree of depth 1, where each leaf has the same value.
With each subsequent gene that is added, the tree is made one deeper.
The ratio in which the probability of each branch is divided over the new leafs is decided by the correlation between the last gene to be added, and the new gene.
If the in the new leaf both genes are of the same expression level, the new probability is
%
\begin{equation}
    p_\mathrm{leaf} = p_\mathrm{parent} \cdot (\frac{1}{n} + (1 - \frac{1}{n}) \cdot r)
\end{equation}
%
where $r$ is the correlation, and $n$ is the number of expression levels.
The leaf value for a mismatch is then defined as
%
\begin{equation}
    p_\mathrm{leaf} = p_\mathrm{parent} \cdot \frac{(1 - (\frac{1}{n} + (1 - \frac{1}{n}) \cdot r))}{n-1} 
\end{equation}
%
This method ensures that the joint PMF remains normalized after each addition, as in the latter equation we simply divide the remaining probability not assigned to the former case equally over the remaining $n-1$ leafs.

% Evolution of the system over time
The evolution of the joint PMF over time is governed by a set of Boolean functions.
These rule function represent edges in a network motif, which define the dynamics through which genes regulate each other.
Most of these edges are one-to-one mappings; these are of the type "gene A activates gene B in the next timestep, if A is activated in the current timestep".
Many-to-one mappings are enabled in the framework, but are rare in real life scenarios.
They are not unimaginable, however.
For instance, gene A might be translated into a promotor for gene B, but only if the co-enzyme for which gene C codes is also present.
Due to the poor scalability of this model, we cannot capture an entire GRN; even in a binary system, the number of leaf-values in the tree structure is $2^k$, where $k$ is the number of genes.
Smaller GRNs contain of around 50 genes typically, which would require a tree too big to process in Python in reasonable time.
To overcome this scaling issue, we isolate a motif from the network.
The rest of the network is inferred through the correlation matrix.
As the correlation matrix is only applied in the first timestep, this model is not suitable for long simulations.

To find the next state of the system, all rules of the motif are applied to the system in the previous state.
If there are no rules acting on a gene, it is assumed that its state remains unchanged.
If multiple rules act on the same gene, the outcome is the output that is produced by the majority of the rules.
In the case of a tie downregulation dominates, similarly as in nature \cite{}.
The result of this is a new $n$-tree describign the joint PMF at time $t=t_0+dt$.

\subsection{Generating random GRNs}
% Generating random networks

In order to test hypotheses regarding the GRN networks that occur in biological systems, we need to make a comparison to the set of all possible GRN motifs.
We aim to sample from this set by generating random GRN motifs.
These should adhere to a set of network properties that are characteristic for GRN motifs, but should otherwise be completely random.
We aim to do so by using the algorithm described in figure~\ref{random_generation}.
In this algorithm, \texttt{rmatrix} denotes the matrix of all correlations between the different genes in the motif.
This defines the initial joint PDF of the system.
The \texttt{GenerateRules} function is given in figure~\ref{generate_rules}, and the \texttt{GenerateCorrelations} function in figure~\ref{generate_correlations}.
The \texttt{FindEdges} function simply makes a list of all possible edges between all nodes.
This function can be set to allow only one-to-one edges, or also many-to-one.
The latter is required to build in complex functions.
% WAIT, can we make an AND function with just plus and minus?
% If so, we can drop all this nonsense

%TODO
%TYPE OF NETWORK, NORMAL RANDOM, NOT BARABASI

\begin{algorithm}[H]
 \SetKwData{Edges}{edges}
 \SetKwData{Rmatrix}{rmatrix}
 \SetKwData{Rules}{rules}
 \SetKwData{GRN}{grn}
 \SetKwData{GRNs}{grns}
 \SetKwData{Rules_cnt}{rules_cnt}
 \SetKwFunction{FindEdges}{FindEdges}
 \SetKwFunction{Pack}{Pack}
 \SetKwFunction{GenerateCorrelations}{GenerateCorrelations}
 \SetKwFunction{GenerateRules}{GenerateRules}

 \Input{motif size, sample size and average indegree}
 \Output{a list of GRN objects}
 \BlankLine

 \Edges $\leftarrow$ \FindEdges{\Nodes} \tcp{find all possibles edges between \Nodes}
 \For{$n \leftarrow 1$ \kwTo sample size}{
   go to next section\;
 }
 \label{random_generation}
 \caption{Generating a random network}
\end{algorithm}

%TODO NETWORK PROPERTIES
In the generation of random networks, we aim to produce results with similar network properties to actual networks.
We decided on an indegree between 2 and 4, which seems typical for the smaller GRN networks \cite{lahdesmaki2003learning}.
Larger networks typically have higher average indegrees, but as Boolean networks are not a good approximation for larger networks this is out of our scope \cite{lahdesmaki2003learning, karlebach2008modelling}.
In general, the indegree of individual genes is low as well, meaning hub formation as in Barabasi-Albers networks is atypical.
The Boolean function often consists predominantly of simple relations, such as gene A downregulates gene B \cite{lahdesmaki2003learning, schlitt2007current}.

%TODO LIBRARY OF FUNCTIONS

\subsection{Parameter Nudging}

%% Nudging
%TODO
% Use information like what DJ uses

This method of nudging is reminiscent of the signal-response curves popular in biological sciences \cite{tyson2010functional}.
These, however, do not carry any mutual information naturally.

\subsection{Quantification Measures}

% Difference of two joint PDF objects
To measure the effect of parameter nudging on a joint PMF, we use the Hellinger distance to quantify the difference between two distributions.
This is defined as
%
\begin{equation}
H(X, Y) = \frac{1}{\sqrt{2}} \sqrt{\sum^k_{i=1} (\sqrt{x_i} - \sqrt{y_i})^2}
\end{equation}
%
where ${x_1 ... x_k}$ are probabilities of states of $X$ occurring, and ${y_1 ... y_k}$ for states of $Y$.

% Mutual information
The mutual information is used to inspect information decay over time in the distribution of states in the GRN system.
A mutual information of zero between the state of the system at $t=0$ and $t=1$ implies that knowledge of the former state provides no insight in the latter.
Similarly, a mutual information equal to the system entropy would imply that everything is known about the system in the latter state when examining the former.
It is implemented as described in Eq.~\ref{MI}, and imported from the jointPDF package \cite{jointpdf}.

% Synergy: WMS
% Synergy: Quax
We use two synergy measures in the testing of our hypotheses.
First, we use the WMS-synergy (Eq.~\ref{WMS}) as a useful lower bound of synergy in the system.
Second, we use the SRV-based synergy measure proposed by Quax et al. (Eq.~\ref{SRV}) \cite{quax2017quantifying}.
The used implementations for both measures are imported from the jointPDF package \cite{jointpdf}.

\subsection{Hypotheses}

We hypothesize that:

% Hypotheses (move to end introduction)
\begin{itemize}
\item There is a correlation between synergy and nudge resilience in random GRN motifs
\item There is significantly more synergy in a biological GRN motif versus a random GRN motif
\item An actual GRN scores significantly better in memory than a random GRN motif
\item An actual GRN scores significantly better in single-nudge resilience than a random GRN motif
\item An actual GRN does not score significantly differently in multiple-nudge resilience to a random GRN motif
\item There is a stronger than linear decrease in resilience when increasing the number of variables nudged in a biological GRN motif
\end{itemize}

% If I have more time...
% \item An actual GRN motif is optimized for memory and resilience

% If I have even more time...
% \item An actual GRN is at the Pareto boundary of the memory/resilience cost function
% \item Synergy is found at a low level in biological networks, the level of common network motifs 
% \item Synergy is found at a low level in trained random GRNs
% \item The (DJ graph) indicates a level of synergistic control that is greater than random

\end{document}
