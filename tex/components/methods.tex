% Version: 0.0

\documentclass[../main.tex]{subfiles}

\begin{document}

\subsection{Complexity Profile}

% Introduction to our synergy profile
% Link to introduction
In previous research, the mutual information within a system has been used to investigate complexity in systems of independent and dependent random variables.
A multiple mutual information-based profile has been proposed in the literature that is able to give insights beyond pairwise relations.
A synergy profile can be considered as a plot of the fraction captured of the total mutual information between all input variables and all output variables versus the number of variables taken into consideration, or
%
\begin{equation}
C_\mathrm{mult}(k) = \frac{1}{\binom{n}{k}}\frac{\sum_{X_i \in [X]^k} [\mathrm{I}(X_i;Y)]}{\mathrm{I}(X;Y)}
\end{equation}

This profile has the property $C_\mathrm{mult}(0) = 0$, as for $k = 0$ there is only the empty set, which has zero mutual information with $Y$.
In addition, we know that $C_\mathrm{mult}(n) = 1$, as this simply results in
%
\begin{align}
C_\mathrm{mult}(k) 
&= \frac{1}{\binom{n}{k}}\frac{\sum_{X_i \in [X]^k} [\mathrm{I}(X_i;Y)]}{\mathrm{I}(X;Y)} \\
&= \frac{\mathrm{I}(X;Y)}{\mathrm{I}(X;Y)} \\
&= 1
\end{align}

Finally, we can show that this profile is non-decreasing.
We can proof this by imagining an extreme case, where out of variable set $Z$ only $z_1$ provides direct information about the output variable, whereas the rest only provide information when all considered together.
When considering a subset size $1 \le k < n $, the complexity will be
%
\begin{equation}
C_\mathrm{mult}(k) = \frac{1}{\binom{k}{n}} \mathrm{I}(z_1;Y)
\end{equation}
%
where $n_z$ is the size of set $Z$.
As the mutual information term is constant for $k$, only the fraction determines the complexity value.
As with increasing $k$ fewer and fewer subsets can be made, the function will always be increasing or stagnant, the latter of which is possible only if $C_\mathrm{mult}(0) = 0$.

As there are many ways to take subsets when $1 < k < n$, we average the sum over all subsets.
However, it is also possible to rewrite this to
%
\begin{equation}
C_\mathrm{mult}(k) = \frac{\max_{X_i \in [X]^k} [\mathrm{I}(X_i;Y)]}{\mathrm{I}(X;Y)}
\end{equation}
%
to focus on extreme values within the set of subsets of size $k$.

% Go into the math behind it, using the NPEET library documentation
We apply this complexity measure on continuous distributions, forcing us to use approximation methods for the mutual information term.
we use the k-Nearest Neighbor method described by Kraskov, and implemented in Python by Ver Steeg et al. \cite{kraskov2004estimating, versteeg2013NPEET}.


% TOT HIER GOED


For this method, a set of $N$ of samples is drawn from the underlying distributions of both the input and the output system.
This results in two matrices: an input sample matrix of size $N \times X$, and an output sample matrix of size $N \times Y$, filled with continuous values.
In both matrices, the $N$-size dimension represents samples, and the $X$- or $Y$-sized dimension represents the variables of the respective system.
These samples are used in the NPEET library for a multiple mutual information approximation.
Let (...)
%
\begin{equation}
\mathrm{Math of the NPEET code}
\end{equation}
%
Then (...)

% Subsets
We have now approximated the total mutual information between the input- and the output system.
To produce a plot, we must obtain a mutual information estimate for subsets of each size $1 \le k \le X$, versus the entire output system.
Too obtain this overall estimate for each subset size, we first find the set of mutual informations between every possible subset of $X$ of size $k$, versius the entire system $Y$.
Then, we take the average of the set and divide by the total mutual information between the input- and output system to arrive at the value that corresponds to subset size $k$ in our synergy profile.
We repeat this system for each possible subset size.
Finally, the plot is produced by plotting the $k$ against the corresponding measure.

\subsection{Synergy Quantification}

% Analytical quantification for small systems

% Approximation for >3 size systems

\subsection{Simulation Methodology}

% General idea
In our simulation, we want to consider the synergetic properties, memory, and resilience of gene regulatory networks.
In a biological system, there is no clear set of input- and output variables.
However, we do have a development of the system through time.
As such, we have chosen to consider the system $A_{t=0}$ as the input system, and system $A_{t=\delta t}$ as the output system.
Here, $\delta t$ is an step in time. % in arbitrary units or in seconds, once we start using rates?
Scalable. More than three needed.

% System definition
We use a continuous model for a gene regulatory system.
The system is defined through a set of independent initial distributions.
For the sake of simplicity, we have assumed these distributions to be random.
We development of the system over time is defined through a set of flux rates.
These (...) % write once I rewrote the system definition to fit the MatLab model
The set of flux rates can be trained, and is ultimately what defines synergy in the system

% System development over time


% Cost function
As a measure for .. we used the relative entropy, the Kullback-Leibler divergence.
We utilized a k-Nearest Neighboor based approximation, as described by Wang \cite{wang2009divergence}.
Estimation is done using k-Nearest Neighbor approximation for mutual information as described by Kraskov \cite{kraskov2004estimating}.
We used the NPEET library implementation for both these properties \cite{versteeg2013NPEET}.
De wiskunde bespreken waarom MI op een deterministisch systeem kan.
Defend if normalization is still necessary if I use the KL-divergence and the MI (I do need to somehow invert the MI, as this should be high, and the divergence small)
% Some information on nudges (methods)
Nudge (noise) is addition of two normal distributions (after all) (noise addition to initial)
A nudge is defined as a small change in the probabilities for the the different states of one variable. To maintain normalization, the overall nudge summed should equal zero.
First of all we focus on local nudges, those that only affect one variable in the system, but in the depending on research progress we can expand this to pertubations that affect multiple variables. 
The impact of the nudge we measure with the difference in the output squared, first of all.

% Generating a random GRN-like network
% Some networkx required

% Training of free parameters
Method used.
Genetic algorithm option.
Best performing BFGS.
Reason is that the energy landscape is highly irregular.
Training is performed on one sample.
Several resamples are done, each time continuing training.

\subsection{Hypotheses}

We hypothesize that:

% Hypotheses (move to end introduction)
\begin{itemize}
\item There is significantly more synergy in a biological GRN motif versus a random GRN motif
\item An actual GRN scores significantly better in memory and single-nudge resilience than a random GRN
\item The is a stronger than linear decrease in resilience when increasing the number of variables nudged
\item An actual GRN is at the Pareto boundary of the memory/resilience cost function
\item Synergy is found at a low level in biological networks, the level of common network motifs 
\item Synergy is found at a low level in trained random GRNs
\end{itemize}

\end{document}