% Version: 1.2
% In the end I decided to do discrete research, if I need the continuous stuff again go back to commits before Aug 2017

\documentclass[../main.tex]{subfiles}

\begin{document}

\subsection{Model design}

\subsubsection{Simulation methodology}
%TODO te wollig en te veel informatie: nog steeds na opschonen?
%TODO last check consistency math notation

% First tell what kind of model we use
As gene regulation is a complicated process of molecular dynamics over time, we are represent reality with a simplified model that behaves similarly to reality.
We use a model with both a discrete time-dimension and discrete expression level to describe a gene regulatory system.
A system consists of $n$ genes, which can be in state $m \in \{0, 1, ..., l\}$.
The value $l$, the number of possible states, can be configured to be any integer value given $l \ge 2$.
When using $l = 2$, this model reduces to the full Boolean model for gene regulation \cite{bolouri2002modeling}.
Any greater value for $l$ allows for more complex state transistions, and effectively yields us a multi-valued logic model.

% A reason why we chose this
We chose this model over an ODE model because of how it provides a naturally constrained sample space.
In an ODE model, there are a great number of configurable parameters.
For each parameter, only a limited range will be able to produce systems that function remotely like realistic systems.
As these ranges are unknown to us and can take any real value, generating realistic random networks would be a challenge.
In this study, sampling random networks is a central part of the experimental design.
A discrete model provides us with a large but finite set, where the size of the sample space is
%
\begin{equation}
|X_\mathrm{total}| = l^{n \cdot l^n}
\end{equation}
%
This makes it easier for us to propose a model that is realistic.

% Second, explain the two representations that we use
We use two distinct representations of gene regulation motifs within this framework.
First, we use a transition table form.
This table consists of a mapping from every possible state ($l^n$ in total) to its corresponding state at $t_\mathrm{next} = t_\mathrm{current} + 1$.
Second, we use a graph form.
In this format, each gene is represented by a node.
Relations between genes are represented as edges between these nodes.
These edges mimics the evolution of the joint PMF over time by functioning as a set of Boolean functions, which we will refer to as 'rule functions'.
These rule functions define the dynamics through which genes regulate each other.

% A bit more detail into the edges
Edges have at least one origin and a single target, which map to the in- and outputs of a logic function.
Most of these edges are one-to-one mappings; these are of the type "gene A activates gene B in the next timestep, if A is activated in the current timestep".
Many-to-one mappings are possible; gene A might be translated into a promotor for gene B, but only if the co-enzyme for which gene C codes is also present.
Many-to-many mappings are not included in our model in the framework, as they can be captured by a set of many-to-one relationships, each with the same inputs and a different output.
The possible edges are:
%
\begin{itemize}
\item Stimulation (+), adds the expression level $m$ of a single source to the target
\item Inhibition (-), subtracts the expression level $m$ of a single source from the target
\item AND-stimulation, adds the minimum expression level $\min(m_i)$ of all sources to the target
\item AND-inhibition, subtracts the minimum expression level $\min(m_i)$ of all sources from the target
\end{itemize}
%
With these edges, this representation mimics the relationships between genes in a natural network.
The AND-variants are designed to mimic co-factors, two gene products that first need to bind to each other before they can simulate or inhibit another gene.
The minimum expression level of the two inputs is returned, as the two gene products only work when formed into a complex.
When a gene is not stimulated, it is assumed that the expression level decays by one every timestep.
It is allowed for in the model for a gene to stimulate itself, which is commonly seen in GRNs \cite{thomas1995dynamical, zhou2016relative}.

% Final thoughtss
A single graph form can correspond to $n!$ different transition tables, depending on how we label the genes in the network.
As all these $n!$ transition tables are in essence the same, we look at all different permutations of the transition table for a single network, and select the top table after sorting.
Now, a graph form always corresponds to a single transition table. 
However, one transition table could still be obtained from several different networks.

\subsubsection{Time evolution}

% Third, explain the time evolution, including the jointpdf
In our study we consider synergetic properties, memory, and resilience of gene regulatory networks.
These properties are measured as the system develops over time.
The distribution of the system over all possible states is defined through a joint probability mass function (PMF), which represents the probability that the system is found in any state at time $t$.
We built upon the implementation of this in the jointPDF-framework \cite{jointpdf}\footnote{We discuss features that were not used in this study in Appendix~\ref{appendix_methods}.}.
In this Python framework, a joint distribution is stored as an $l$-tree of depth $n$, where $l$ is the number of states a variable can be found in and $n$ is the motif size.
We can compare two PMFs at two different points in time, each representing a distribution of system states at that time.
As such, we define the system $\mathbf{X}_{t=0}$ as the input system, and system $\mathbf{X}_{t=\delta t}$ as the output system.
Here, $\delta t$ is an step in time in arbitrary units, where we usually choose the value $\delta t = 1$. 
The jointPDF-framework supports the generation of a new joint PMF from a starting PMF paired with a transition table.
As a result, we use a transition table for a deterministic time evolution of the distribution of system states.
The result of this is a new $l$-tree describing the joint PMF at time $t=t_0+\delta t$.

% Turning a network into a transition table
Building a transition table from a network representation of a GRN motif is not trivial.
To find the next state of the system, all rules functions of the motif are applied to the system in the previous state.
If multiple rules act on the same gene, the outcome is the output that is determined using a 'deciding condition'.
The condition we use, the \texttt{totaleffect}-condition, sums the outputs of all the functions affecting one gene, with the added condition that the expression level remains within ($0 \le m < l$).
An implication of this is that one strong inhibitor can overpower several weak stimulators completely\footnote{All supported deciding conditions are listed in Appendix~\ref{alternative_deciding}}.
We build a transition table by determining for each possible state of the system what the next state would be.
It is noted that a self-link is defined to be always active, regardless of its states.
This is commonly done in the literature as well.
If there are no rules acting on a gene, it is assumed that its expression level decreases by one due to chemical decay.
A side effect of this is that, while the timesteps are in arbitrary units, the resolution in the time dimension becomes higher with higher-valued logic.
After all, in a Boolean model a gene decays from active to non-active in 1 timestep, whereas in 5-valued logic this will take 5 steps.

% Limitations of the model
The size of the set of genes $\mathbf{X}$ entered in the model is in theory arbitrarily big, but in reality limited by computational complexity of the evaluation of the model.
The time evolution of the model runs in $O(l^{c \cdot n})$, as each leaf-value of the $n$-depth tree needs to be evaluated, where $c$ is a constant. 
% The WMS syenrgy takes N times MI, MI takes c times entropy, entropy uses marginalize and then does some operations that are probably O(c N

\subsubsection{Initialization of the PDF}

% Go a bit more into initializing jointPDF
To initialize a jointPDF-object that represents a system of genes, we need to determine an initial distribution of system states.
The jointPDF Python package offers both a uniform and random initialization.
However, in the event that we want to insert a real motif into the model, we added the option to initialize the jointPDF-object in a manner that represents the prevalence of true system states.
For real GRNs, we often have some data about correlations between genes available from empirical studies \cite{ideker2001integrated}.
For instance, gene $X_0$ and gene $X_1$ might rarely be activated together.
Our model can be configured to use a set of gene-to-gene correlations, provided in the form of a correlation matrix, and base an initial distribution on this.
In this particular scenario, the PMF will be close to zero for all states where gene $X_0$ and gene $X_1$ are activated together.

% Explain how to get a PMF from a correlation matrix
Due to the poor scalability of this model, we cannot capture an entire GRN; even in a binary system, the number of leaf-values in the tree structure is $2^k$, where $k$ is the number of genes.
Smaller GRNs contain of around 50 genes typically, which would require a tree too big to process in Python in reasonable time.
To overcome this scaling issue, we isolate a motif from the network.
The rest of the network is inferred through the correlation matrix.
As the correlation matrix is only applied in the first timestep, this model is not suitable for simulations of many timesteps.

In this study, we focus on generating random networks, with random initial correlations.
The key difference between a joint PMFs with correlations as opposed to a uniform PMF is that the initial entropy is lower; after all, the uncertainty in the distribution is maximized if all states have an equal chance of occuring.
Real GRN networks are extremely unlikely to have each state be equally likely to occur.
As such, the primary goal is to produce initial joint PMFs that are lower in entropy than a uniform distribution, and are closer in entropy to similar real-world motifs. 
For this purpose we do not need a sophisticated model that samples and applies all possible first-order correlations in the network.
We assume a system that can be rewritten as a linear system, where each gene correlates with the previous and the next gene in the system.
As the order of the labels is arbitrary, in practice we assume that our system can be written as a linear system.
This implies that each gene is correlated with at most two other genes, and that there are no correlation loops\footnote{This is not realistic, but a practical limitation of our model with minimal impact on our results. We discuss this limitation further in section~\ref{sec:discussion}, and discuss future improvements.}.

The correlation list is converted to a joint PMF by assuming that the first gene has an equal chance of being in each state.
The result is a tree of depth 1, where each leaf has the same value.
With each subsequent gene that is added, the tree is made one layer deeper.
The ratio in which the probability of each branch is divided over the new leafs is decided by the correlation between the last gene to be added, and the new gene.
The new probability is
%
\begin{equation}
\label{parentchild}
 \Pr\left( m_\mathrm{child} \given m_\mathrm{parent} \right) = 
    \begin{cases} 
    \Pr(m_\mathrm{parent}) \cdot (\frac{1}{l} + (1 - \frac{1}{l}) \cdot r) &  m_\mathrm{child} = m_\mathrm{parent} \\
    \Pr(m_\mathrm{parent}) \cdot \frac{(1 - (\frac{1}{l} + (1 - \frac{1}{l}) \cdot r))}{l-1} &  m_\mathrm{child} \neq m_\mathrm{parent} \\
    \end{cases}
\end{equation}
%
where $r$ is the correlation between the parent- and child-gene.
The parent gene is the gene with the index preceding the child gene.
The correlation here is simply defined as the percentage chance that the child is in the same state as its parent.
This method ensures that the joint PMF remains normalized after each addition, as in the latter equation we simply divide the remaining probability not assigned to the former case equally over the remaining $l-1$ children.

\subsubsection{Generating random state transitions}

As a control group, we generate completely random state transitions, representing a completely random system.
First, a correlation matrix describing all correlations between the different genes in the motif is randomly generated.
This defines an initial distribution of states for our random motif, and represents the influence of the part of the GRN that is not part of the motif.
We use a Python implementation of the vine method, an algorithm to generate random correlation matrices with large off-diagonal values \cite{lewandowski2009generating}.
We then use values on the band above the diagonal as our correlation list, as specified in Eq.~\ref{parentchild}. %Rick: you mean you consider only corerlations between x and x+1? Not x and x+2? Me: yes, not ideal but good enough given time constraints.

For the sake of comparison, we sample from the set of all possible transition tables.
We use a completely randomized design\footnote{Implementing variance reduction is not impossible, but requires a non-standard design that we will elaborate in section~\ref{sec:discussion}.}.
A transition table can be represented by a base-$l$ number, padded with zeroes to the left until it is of length $n \cdot l^n$.
The resulting list of digits represents the expression level of all the genes on the side of our transition table that describes the future state of the system.
To sample a random number, we draw a random integer for each digit where $0 \le x_\mathrm{random} < l$.
As long as the present states are always represented in the same order in the transition table, this allows us to sample any transition table that is possible.

\subsubsection{Generating random biologically realistic GRNs}

We compare the previously generated completely random systems with randomly generated biologically realistic systems.
In contrast drawing samples from the set of all possible transition tables, we also want to draw a sample from the sub-samplespace of all biologically possible transition tables.
These should adhere to a set of network properties that are characteristic for GRN motifs, as well as be constructable from the stimulation and inhibition rules that exist in GRNs, but should otherwise be completely random.
In order to construct these transition tables, we start by constructing a random GRN in graph form.
This algorithm can be configured by defining a set of possible rules, a number of nodes, a fraction of the edges that should be 1-to-1, and a set of possible indegrees for the motifs.

Again, we start by generating a correlation list.
Then, a list of all possible edges is generated.
We limit many-to-one connections to 2-to-1; any higher number of inputs is hard to explain biologically, as it would require three or more gene products to form a complex together.
An edge also is defined to always have only one target, as a many-to-many edge can be rewritten as multiple many-to-one edges.
We do allow self-loops, as these do occur in nature.

With this list, the network generation process is started.
The network is constructed using a scheme related to the Erdős–Rényi algorithm.
A scale-free network (such as a Barabási–Albert network) would have been preferable, but generating one that allows some key characteristics of GRNs is not trivial.
For instance, the Barabási–Albert does not naturally deal with 2-to-1 edges, cannot create cycles in directed graphs, and will not create edges with the same node as the source and target.
An Erdős–Rényi model is a valid choice, as some sources state that this network model applies to gene regulation in some cases in addition to its scale-free counterpart.
In the literature, the Barabási–Albert seems to be a slightly more common choice.
In addition, we argue that the choice does not make a large difference in the results, as our networks are typically so small that there is little difference between the two.

The Erdős–Rényi algorithm from itself is not equipped to deal with 2-to-1 edges.
To solve this problem, we effectively apply the Erdős–Rényi algorithm two times; once for 1-to-1 edges, once for 2-to-1 edges.
Our first step is to determine the desired indegree during each pass.
If we have a $p_\mathrm{1-to-1}$ that describes the desired fraction of 1-to-1 edges, we find that
%
\begin{equation}
k_\mathrm{1-to-1} = k_\mathrm{total} \cdot p_\mathrm{1-to-1}
\end{equation}
%
and
%
\begin{equation}
k_\mathrm{2-to-1} = k_\mathrm{total} \cdot (1 - p_\mathrm{1-to-1})
\end{equation}
%
given that an edge is either 1-to-1 or 2-to-1 (implying that $p_\mathrm{2-to-1} = 1 -  p_\mathrm{1-to-1}$).
From this information, we can calculate the accept probabilities for edges in both the 1-to-1 set, and the 2-to-1 set.
The distribution of the indegree of a node follows a binomial distribution, giving us for both sets the distributions
%
\begin{equation}
\Pr_\mathrm{1-to-1} \left( \mathrm{deg}(v) = k \right) = \binom{n}{k} p_\mathrm{1-to-1}^k (1 - p_\mathrm{1-to-1})^{n-k}
\end{equation}
%
and
%
\begin{equation}
\Pr_\mathrm{2-to-1} \left( \mathrm{deg}(v) = k\right) = \binom{\binom{n}{2}}{k} p_\mathrm{2-to-1}^k (1 - p_\mathrm{2-to-1})^{\binom{n}{2} - k}
\end{equation}
%
We have to keep in mind that we allow self-referring edges, implying that there are $n$ possible 1-to-1 edges per node.
We do not consider an 2-to-1 edge where both origins are the same as a valid edge, implying here are $n^2 - n$ possible 2-to-1 edges.
We can use the mean of a Binomial distribution to calculate the average indegree, which should match the previously calculated average indegrees.
This way, we arrive at the acceptance probabilities of
%
\begin{equation}
p_\mathrm{1-to-1} = \frac{k_\mathrm{1-to-1}}{n}
\end{equation}
%
and
%
\begin{equation}
p_\mathrm{2-to-1} = \frac{k_\mathrm{2-to-1}}{\binom{n}{2}}
\end{equation}
%
Having calculated the parameters for the Erdős–Rényi, we execute the algorithm two times; once for 1-to-1 edges, once for 2-to-1 edges.
Once an edge has been selected, a random function is attached to it, such as inhibition or stimulation.

\subsubsection{Parameter nudging}

% General nudging
The method of nudging used was based on an implementation by Riesthuis \cite{DJ_repository}.
We pass our motifs, a list of variables that are to be nudged, and a nudge size $0 \le \epsilon \le 1$ that represents the fraction of the total probability that should be moved as part of the nudge.
The introduce a local nudge, implying that the joint PDF of the variables that are not nudged remains unaltered.

% How a nudge works
The nudge is applied as follows.
First, we produce the joint probability matrix $z$ for the each possible configuration of our non-nudged variables.
For instance, if we apply a nudge on the gene $X_0$ in a system with $n=2$ and $l=2$, we might find $z = [0.2, 0.4]$.
To this matrix $z$, a random nudge matrix of the same shape is added where the total sum is zero, and the absolute sum being equal to $2 \epsilon \times \sum z$. 
The nudge vector is configured in such a way that probabilities always fall in the range $0 \le p \le 1$ after the nudge is applied.
We do this twice in this example, as the first only covers the system states where $m_{X_1} = 0$.
The nudged version of the matrix $z^\prime$ is plugged back into the jointPDF-object.

In practice, a nudge of $\epsilon \ge \frac{1}{l^{n_\mathrm{nudged}}}$ is not safe to use, where $n_\mathrm{nudged}$ is the number of genes nudged and $l^{n_\mathrm{nudged}}$ is the number of values in $z$.
If we do apply this nudge, it cannot be guaranteed that the nudge can be applied while keeping probabilities in the range $0 \le p \le 1$ after the nudge.
For example, imagine a trivial system with $n = 1$, $l = 2$ and the distribution $[0.5, 0.5]$.
We cannot apply a nudge of $\epsilon = 0.6$, as this would require us to create a nudge vector $[0.6, -0.6]$ or $[-0.6, 0.6]$.
Either nudge would leave the system in a state with a negative probability.

\subsection{Analytical methods}

\subsubsection{Quantification measures}

% Difference of two joint PDF objects
To measure the effect of parameter nudging on a joint PMF, we use the Hellinger distance to quantify the difference between two distributions.
This is defined as
%
\begin{equation}
\mathrm{H}\left( X, Y\right) = \frac{1}{\sqrt{2}} \sqrt{\sum^k_{i=1} (\sqrt{x_i} - \sqrt{y_i})^2}
\end{equation}
%
where ${x_1 ... x_k}$ are probabilities of states of $X$ occurring, and ${y_1 ... y_k}$ for states of $Y$.
We apply the nudge to the system at $t = 0$, and compare the nudged and unnudged system at time $t = \delta t$.
When testing the sensitivity to nudges, we always take the average Hellinger distance after applying every possible nudge of the correct 'width'.
For instance, when we apply a nudge on a single target in a system of 4 genes we take the average of 4 values.
This prevents interference of randomness in picking a target gene in our result.
We chose the Hellinger distance instead of the Kullback-Leibler divergence as the latter cannot handle zeros.
Our distributions frequently develop states that have zero probabilities, as GRNs can have unstable states that cannot be reached from any other state.

% Mutual information
We quantify the memory of a system using the mutual information $\mathrm{I}\left(\mathbf{X}_t ; \mathbf{X}_{t + \delta t}\right)$.
As no effects are taken into account from outside the system, this also represents the magnitude of causal effects in the system.
A mutual information of zero between $\mathbf{X}_t$ and $\mathbf{X}_{t + \delta t}$ implies that knowledge of the former state provides no insight in the latter.
Similarly, a mutual information equal to the system entropy would imply that everything is known about the system in the latter state when examining the former.
It is implemented as described in Eq.~\ref{MI}, and imported from the jointPDF package \cite{jointpdf}.
We normalize this by dividing by $\mathrm{H}\left(\mathbf{X}_{t + \delta t}\right)$, which yields a measure bounded by zero and one.

% Synergy: WMS
% Synergy: Quax
We found that the SRV-based synergy measure proposed by Quax et al. (Eq.~\ref{SRV}) scales poorly with motif size larger than 2 genes \cite{quax2017quantifying}.
As a result we use a simpler synergy measure that is based on the average between an upper- and lower bound estimate for the amount of synergy.
We use the WMS-synergy (Eq.~\ref{WMS}) as a lower bound of synergy in the system.
This is a computationally cheap measure, and as the systems we measure synergy in are small the intrinsic error in this measure should not be too large.
For an upper bound, we use the maximum entropy of a single element of the system and the entropy of the entire system
%
\begin{equation}
\mathrm{I}\left( \mathbf{X}_{t=0}; \mathbf{X}_{t=\delta t} \right) - \max_i [\mathrm{I}\left( X_{t=0,i};\mathbf{X}_{t=\delta t}\right)]
\end{equation}
%
where $x_i$ is the $i$-th element of the system, and $X$ is the full system.
This is a variant of the $\mathrm{I}_\mathrm{max}$-synergy described in section~\ref{synergy} which avoids the use of the specific surprise, as this quantity is not readily computed when the set of predicted variables has an arbitrary size.
This resembles the WMS-synergy lower bound, but whereas the WMS-synergy assumes that there is no redundancy between the elements in the system, this measure assumes there is full redundancy.
As a result, this measure assumes that all information that does not originate from the system component with the largest MI with the predicted system is synergistic in nature, creating an upper bound for synergy.
The used implementation for the WMS synergy is imported from the jointPDF package \cite{jointpdf}.
We normalize this to fall between 0 and 1 by dividing by $\mathrm{I}\left( \mathbf{X}_t ; \mathbf{X}_{t + \delta t}\right)$.

\subsubsection{Sample space visualization}

An important sanity check is to verify that the sample of biologically possible transition tables shows that this is a subspace of all possible transition tables.
To achieve this, we consider every gene's expression level in every future state of the transition table as an independent variable.
We then create a 2-dimensional embedding of this vector of random variables using t-Distributed Stochastic Neighbor Embedding (t-SNE), a form of dimensionality reduction that works well on datasets with many variables per datapoint \cite{maaten2008visualizing}.
All datapoints that are cast in this 2-dimensional embedding are colored to indicate whether they are part of the biologically possible set, or the completely random set.
If a portion of the datapoints of one set are clearly not mixing with the other, it is implied that this set covers a part of the sample space that the other does not.
We used a perplexity of 10, which is below the usually recommended value by the Scikit Learn.
This was done as we found that for higher values the datapoints clumped up too much.

\subsubsection{Cycle finding}

Cyclical sequences of state transitions are a key element of biological networks, and can be found in real GRN motifs without considering the rest of the network \cite{burda2011motifs}.
To recognize cycles, we include a cycle-finder in the our framework.
The input for this function is a motif of either type, as well as a maximum cycle length $N_\mathrm{cycle,max}$.
The maximum length, naturally, is limited in theory by the number of possible states.
We work with deterministic systems, so for each state a subsequent state is defined.
As a result, a cycle can at most visit every state once, giving it a length equal to the number of possible states ($N_\mathrm{states} = l^n$).
For each possible initial state we do $N_\mathrm{cycle,max}$ time evaluation.
If we return to the original state, we save this sequence of states as a cycle.
If we encounter a state that is already in a known cycle, we do not return the same cycle twice.
The return value is a list of cycles, each captured in a list of states.
A loop of size 1 is a point attractor, a larger loop is a cyclical attractor. 

\subsubsection{Complexity profile}

% Introduction to our synergy profile
% Link to introduction
In previous research, the mutual information within a system has been used to investigate complexity in systems of independent and dependent random variables.
A multiple mutual information-based profile has been proposed in the literature that is able to give insights beyond pairwise relations.
A synergy profile can be considered as a plot of the fraction captured of the total mutual information between all input variables and all output variables versus the number of variables taken into consideration, or
%
\begin{equation}
C_\mathrm{mult}(k) = \frac{1}{\binom{n}{k}}\frac{\sum_{X_i \in [\mathbf{X}]^k} [\mathrm{I}\left( X_i;Y \right)]}{\mathrm{I}\left( \mathbf{X};Y\right)}
\end{equation}

This profile has the property $C_\mathrm{mult}\left( 0 \right) = 0$, as for $k = 0$ there is only the empty set, which has zero mutual information with $Y$.
In addition, we know that $C_\mathrm{mult}\left( n\right) = 1$, as this simply results in
%
\begin{align}
C_\mathrm{mult}\left( k \right)  
&= \frac{1}{\binom{n}{k}}\frac{\sum_{X_i \in [\mathbf{X}]^k} [\mathrm{I}\left( X_i;Y\right)]}{\mathrm{I}\left( \mathbf{X};Y\right)} \\
&= \frac{\mathrm{I}\left( \mathbf{X};Y\right)}{\mathrm{I}\left( \mathbf{X};Y\right)} \\
&= 1
\end{align}

Finally, we can show that this profile is non-decreasing.
We can prove this by imagining an extreme case, where out of variable set $Z$ only $z_1$ provides direct information about the output variable, whereas the rest only provide information when all considered together.
When considering a subset size $1 \le k < n_z $, the complexity will be
%
\begin{equation}
C_\mathrm{mult}\left( k \right)  = \frac{1}{\binom{k}{n_z}} \mathrm{I}\left( z_1;Y\right)
\end{equation}
%
where $n_z$ is the size of set $Z$.
As the mutual information term is constant for $k$, only the fraction determines the complexity value.
As with increasing $k$ fewer and fewer subsets can be made, the function will always be increasing or stagnant, the latter of which is possible only if $C_\mathrm{mult}\left( 0 \right) = 0$.

As there are many ways to take subsets when $1 < k < n$, we average the sum over all subsets.
However, it is also possible to rewrite this to
%
\begin{equation}
C_\mathrm{mult}\left( k \right) = \frac{\max_{X_i \in [\mathbf{X}]^k} [\mathrm{I}\left( X_i;Y \right)]}{\mathrm{I}\left( \mathbf{X};Y\right)}
\end{equation}
%
to focus on extreme values within the set of subsets of size $k$.
In the discrete case, we can simply calculate the mutual information as in Eq.~\ref{MI}\footnote{For a continuous case a complexity profile can be constructed using use k-Nearest Neighbor method described by Kraskov \cite{kraskov2004estimating}}.

% Subsets
We now can approximate the total mutual information between the input- and the output system.
To produce a plot, we must obtain a mutual information estimate for subsets of each size $1 \le k \le X$, versus the entire output system.
Too obtain this overall estimate for each subset size, we first find the set of mutual informations between every possible subset of $X$ of size $k$, versus the entire system $Y$.
Then, we take the average of the set and divide by the total mutual information between the input- and output system to arrive at the value that corresponds to subset size $k$ in our synergy profile.
We repeat this system for each possible subset size.
Finally, the plot is produced by plotting the $k$ against the corresponding measure.

\subsection{Experimental design}

\subsubsection{Hypotheses}

We hypothesize that:

\begin{enumerate}
\itemsep0em 
\item There is a positive correlation between synergy and nudge resilience in random motifs
\item There is a positive correlation between synergy and nudge resilience in biologically possible motifs
\item There is a positive correlation between synergy and system memory in random motifs
\item There is a positive correlation between synergy and system memory in biologically possible motifs
\item There is significantly more synergy in a biologically possible motif than in a random motif
\item A biologically possible motif scores significantly better in memory than a random motif
\item A biologically possible motif scores significantly better in single-nudge resilience than a random GRN motif
\item A biologically possible GRN does not score significantly differently in multiple-nudge resilience to a random GRN motif
%\item There is a stronger than linear decrease in resilience when increasing the number of variables nudged in a biological GRN motif
\end{enumerate}
% Rick: dus je hebt eigenlijk wel DRIE populaties van PMFs die je considert, niet slechts twee...? Me: Nee?

To support these tests, we visualize the distribution of both biologically possible- and completely random motif in 3D-space, with axes corresponding to the synergy, the system memory, and the system nudge resilience.
We also produce all 2D projections from this 3D distribution of our motifs, three in total, leaving out one of the three variables in each.
We calculate both the bivariate correlation and the partial correlation for hypotheses (1)-(4).
In addition, we also produce ensemble of complexity profiles for each experiment, in order to compare the distribution of synergy and redundancy in both random and biologically possible systems.

We perform several sanity checks to validate our model.
We check that the biologically feasible motifs are a sub-samplespace through a t-SNE visualization.
We also look at the prevalence of cycles in both samples, as well as how common several real network motifs are in the samples.
In both cases we expect a higher rate of occurence in the biologically feasible motifs.
\footnote{We also validated the experimental design by running a few transition tables through the model of which we know the expected result. An example is the X-OR, which should have high synergy and high resilience. This validation is included in the code section of the repository \url{github.com/dgoldsb/synergy-jointpdf}.}

\subsubsection{Parameter ranges}

We perform a parameter sweep over several key parameters.
These parameters could be narrowed down to a range of interest, either due to limits regarding the time complexity of increasing it further, or by using ranges specified in the literature.
The ranges, along with the increments with which we increase in our sweep, are shown in Table~\ref{parameters}.
As part of the experiments all possible numbers of genes targeted by a nudge are evaluated, thus this was excluded from this table.
We decided on an average indegree of 4, which seems typical for the smaller GRN networks \cite{lahdesmaki2003learning}.
With this indegree, along with a scale-free design, the random motifs will have similar network properties to actual networks.
Larger networks typically have higher average indegrees, but as Boolean networks are not a good approximation for larger networks this is out of our scope \cite{lahdesmaki2003learning, karlebach2008modelling}.
The nudge size was limited to 80\% of the maximum safe nudge in the parameter sweep, as for higher values the nudging function stopped performing well.
In many cases, the nudged PDF would not remain close to normalized.


\begin{table}
\begin{tabular}{| l | c | c | c |}
\hline
Parameter & Start & End & Increment \\
\hline
Network size (\#) & 2 & 5 & 1 \\
Logic size (\#) & 2 & 5 & 1 \\
Nudge size (fraction of probability) & 0.1 & 0.4 & 0.15 \\
\hline
\end{tabular}
\centering
\caption{The parameter ranges used for the experiments}
\label{parameters}
\end{table}

In addition to these parameter ranges, we utilize the \texttt{totaleffect} transition function decision rule, and a chance for a 1-to-1 edge of 75\%.
This value is chosen as an educated guess, as we did not find a reasonable statistic in the available literature.
\end{document}
