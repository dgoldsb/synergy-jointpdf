% Version: 0.0

\documentclass[../main.tex]{subfiles}

\begin{document}

\subsection{Synergy Profile}

% Introduction to our synergy profile
% Link to introduction
In previous research, the mutual information within a system has been used to investigate complexity in systems of independent and dependent random variables.
Quax et al. have proposed a multiple mutual information-based profile, that is able to give insights beyond pairwise relations \cite{quax2017quantifying}.
A synergy profile can be considered as a plot of the fraction captured of the total mutual information between all input variables and all output variables, versus the number of variables taken into consideration.
Mutual information can tell us something about synergy in the system, as this expresses itself as negative mutual information (as opposed to redundancy).
When considering zero variables, you have no information on the system, so here the mutual information is 0.
When considering all variables everything is known, so here the fraction of mutual information captured is 1. 
The shape of the curve inbetween tells us something about the presence of synergy within the system.
For instance, if the curve stays low for a long time and then shoots up we can conclude that the system contains a lot of high-level synergy.
As there are many ways to order the variables, we can make the decision to at each position take the variable that adds the maximum amount of information, or to take the mean of all possible curves.

% Go into the math behind it, using the NPEET library documentation
Let us consider the synergy profile of a continuous system with a set of random input variables $A_i$, with input size $X$, and a set of random output variables $B_i$, with output size $Y$.
First, we determine the amount of mutual information between the two sets $\mathrm{I}(A, B)$.
This is estimated using the k-Nearest Neighbor method described by Kraskov, and implemented in Python by Ver Steeg et al. \cite{kraskov2004estimating, versteeg2013NPEET}.
For this method, a total of $N$ of samples is drawn from the underlying distributions of both the input and the output system.
This results in two matrices: an input sample matrix of size $N \times X$, and an output sample matrix of size $N \times Y$, filled with continuous values.
In both matrices, the $N$-size dimension represents samples, and the $X$- or $Y$-sized dimension represents the variables of the respective system.
These samples are used in the NPEET library for a multiple mutual information approximation.
Let (...)

\begin{equation}
\mathrm{Math of the NPEET code}
\end{equation}

Then (...)

% Subsets
We have now approximated the total mutual information between the input- and the output system.
To produce a plot, we must obtain a mutual information estimate for subsets of each size $1 \le k \le X$, versus the entire output system.
Too obtain this overall estimate for each subset size, we first find the set of mutual informations between every possible subset of $X$ of size $k$, versius the entire system $Y$.
Then, we take the average of the set and divide by the total mutual information between the input- and output system to arrive at the value that corresponds to subset size $k$ in our synergy profile.
We repeat this system for each possible subset size.
Finally, the plot is produced by plotting the $k$ against the corresponding measure.

\subsection{Synergy Quantification}

% Analytical quantification for small systems

% Approximation for >3 size systems

\subsection{Simulation Methodology}

% General idea
In our simulation, we want to consider the synergetic properties, memory, and resilience of gene regulatory networks.
In a biological system, there is no clear set of input- and output variables.
However, we do have a development of the system through time.
As such, we have chosen to consider the system $A_{t=0}$ as the input system, and system $A_{t=\delta t}$ as the output system.
Here, $\delta t$ is an step in time. % in arbitrary units or in seconds, once we start using rates?
Scalable. More than three needed.

% System definition
We use a continuous model for a gene regulatory system.
The system is defined through a set of independent initial distributions.
For the sake of simplicity, we have assumed these distributions to be random.
We development of the system over time is defined through a set of flux rates.
These (...) % write once I rewrote the system definition to fit the MatLab model
The set of flux rates can be trained, and is ultimately what defines synergy in the system

% System development over time


% Cost function
As a measure for .. we used the relative entropy, the Kullback-Leibler divergence.
We utilized a k-Nearest Neighboor based approximation, as described by \citep{wang2009divergence}.
Estimation is done using k-Nearest Neighbor approximation for mutual information as described by \citep{kraskov2004estimating}.
We used the NPEET library implementation for both these properties \cite{versteeg2013NPEET}.
De wiskunde bespreken waarom MI op een deterministisch systeem kan.
Defend if normalization is still necessary if I use the KL-divergence and the MI (I do need to somehow invert the MI, as this should be high, and the divergence small)
% Some information on nudges (methods)
Nudge (noise) is addition of two normal distributions (after all) (noise addition to initial)
A nudge is defined as a small change in the probabilities for the the different states of one variable. To maintain normalization, the overall nudge summed should equal zero.
First of all we focus on local nudges, those that only affect one variable in the system, but in the depending on research progress we can expand this to pertubations that affect multiple variables. 
The impact of the nudge we measure with the difference in the output squared, first of all.

% Training of free parameters
Method used.
Genetic algorithm option.
Best performing BFGS.
Reason is that the energy landscape is highly irregular.
Training is performed on one sample.
Several resamples are done, each time continuing training.

\subsection{Hypotheses}

% Hypotheses (move to end introduction)
Make hypothesis maximum vs. average different.

\end{document}