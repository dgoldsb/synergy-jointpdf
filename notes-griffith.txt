synergy is simply that a set of variables COOPERATING together predict a random variable
interest in computational biology is great; many phenotypical traits are not coded by one gene, but by a set of COOPERATING genes

early synergy meaures start from I (upsidedown U), the redundancy measure, and go from there
synergy in turn has an I U measure

have a notation section at the start of my thesis, very important!

the venn diagram we discuss is called a partial information diagram
a simple pi diagram has a surface equal to the MI between the predicting variables and the predicted variable, 
it is possible for a slice of information to me synergistic of 12 and redundant with 3
all information is unique, synergistic or redundant
synergy can be at different levels as {1 2 3} is a higher emergence level than {1 2}
use the term emergence level, let this recur later

griffith gives a nice example for redundant information (replica), synergistic information (XOR) and unique information (individual copy)
synergy disappears if it is made redundant, but does not disappear if one of the variables part of the synergy is duplicate

Make a checklist table for desireable properties of predictors
Smax: invariant to duplicate predictors, between 0 and the MI(X:Y), overestimates as it assumes unique information is synergy when multiple predictors have unique information on the target

(also add predictor and target to notation section, plus terminology GRN, biologically possible (afkorting bedenken?))

WMS synergy: not between 0 and MI(X:Y), negative is redundancy, plus is synergy, definition is the total MI minus the individual MIs, if there is tons of redundancy this becomes negative, underestimates synergy becoming loser with higher n because it n-subtracts n-redundancy (THIS IS WHY IT GETS WORSE)

Correlation importance: duplicate predictors decrease synergy, measures something different from synergy, does not always fall in the upper-lower range we define, is between 0 and MI(X:Y)

Recapping, one is =<, other =>, last !=

Synergistic mutual information: idea is whole minus union, instead of minus sum (the sum causes the duplication!), this union information does not exist, global positivity, self-redundancy, invariant to reordirng, monoticity (an information poorer addition does not upset), target monoticity, right range, between upper and lower bound... but relies on a not analytically solveable formula
key idea; synergy is how much the whole exceeds the UNION of its parts
minimization is the problem, best we can do is a numerical algorithm
 